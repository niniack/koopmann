{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from plotly.subplots import make_subplots\n",
    "from safetensors import safe_open\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "from analysis.utils import load_autoencoder, load_model\n",
    "from koopmann import aesthetics\n",
    "from koopmann.data import DatasetConfig, get_dataset_class\n",
    "from koopmann.models import ConvResNet\n",
    "from koopmann.utils import set_seed\n",
    "from scripts.train_ae.shape_metrics import prepare_acts, undo_preprocessing_acts\n",
    "\n",
    "set_seed(21)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lotusroot\"\n",
    "model_name = f\"resmlp_{dataset_name}\"\n",
    "file_dir = \"/Users/nsa325/koopmann_model_saves\"\n",
    "data_root = \"/Users/nsa325/datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"lotusroot\" or dataset_name == \"yinyang\":\n",
    "    dim = 20\n",
    "    scale_idx = 1\n",
    "    k_steps = 100\n",
    "    flavor = \"exponential\"\n",
    "elif dataset_name == \"mnist\":\n",
    "    dim = 800\n",
    "    scale_idx = 1\n",
    "    k_steps = 10\n",
    "    flavor = \"exponential\"\n",
    "elif dataset_name == \"cifar10\":\n",
    "    dim = 1_000\n",
    "    scale_idx = 1\n",
    "    k_steps = 5000\n",
    "    flavor = \"exponential\"\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "ae_name = f\"dim_{dim}_k_{k_steps}_loc_{scale_idx}_{flavor}_autoencoder_{dataset_name}_model\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_metadata = load_model(file_dir, model_name)\n",
    "model.eval().hook_model().to(device)\n",
    "print(\"Model: \", model_metadata)\n",
    "\n",
    "autoencoder, ae_metadata = load_autoencoder(file_dir, ae_name)\n",
    "autoencoder.eval().to(device)\n",
    "new_dim = ae_metadata[\"in_features\"]\n",
    "preprocess = ae_metadata[\"preprocess\"]\n",
    "K_matrix = autoencoder.koopman_weights.T\n",
    "print(\"Autoencoder: \", ae_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "    dataset_name=model_metadata[\"dataset\"],\n",
    "    num_samples=3_000,\n",
    "    split=\"train\",\n",
    "    seed=42,\n",
    ")\n",
    "DatasetClass = get_dataset_class(name=dataset_config.dataset_name)\n",
    "dataset = DatasetClass(config=dataset_config, root=data_root)\n",
    "\n",
    "subset_size = None\n",
    "if subset_size:\n",
    "    subset_indices = list(range(0, subset_size))\n",
    "    subset = Subset(dataset, subset_indices)\n",
    "\n",
    "batch_size = 3_000\n",
    "batch_size = min(subset_size, batch_size) if subset_size else batch_size\n",
    "dataloader = DataLoader(subset if subset_size else dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict = {}\n",
    "with safe_open(\n",
    "    f\"{file_dir}/{ae_name}_preprocessing.safetensors\", framework=\"pt\", device=\"cpu\"\n",
    ") as f:\n",
    "    for k in f.keys():\n",
    "        preproc_dict[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Preprocess activations?: {preprocess}\")\n",
    "orig_act_dict, proc_act_dict, _ = prepare_acts(\n",
    "    data_train_loader=dataloader,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    svd_dim=ae_metadata[\"in_features\"],\n",
    "    whiten_alpha=preproc_dict[\"wh_alpha_0\"],\n",
    "    preprocess=preprocess,\n",
    "    preprocess_dict=preproc_dict,\n",
    "    only_first_last=True,\n",
    ")\n",
    "proc_act_dict = orig_act_dict if not preprocess else proc_act_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    images, labels = next(iter(dataloader))\n",
    "    labels = labels.squeeze()\n",
    "    model_pred = model(images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = list(orig_act_dict.keys())[0]\n",
    "final_idx = list(orig_act_dict.keys())[-1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = orig_act_dict[init_idx]\n",
    "    x_proj = proc_act_dict[init_idx]\n",
    "\n",
    "    y = orig_act_dict[final_idx]\n",
    "    y_proj = proc_act_dict[final_idx]\n",
    "\n",
    "    if preprocess:\n",
    "        x_unproj = undo_preprocessing_acts(x_proj, preproc_dict, init_idx, device)\n",
    "        y_unproj = undo_preprocessing_acts(y_proj, preproc_dict, final_idx, device)\n",
    "    else:\n",
    "        x_unproj = x_proj\n",
    "        y_unproj = y_proj\n",
    "\n",
    "    # Reconstruct first act\n",
    "    x_proj_obs = autoencoder.encode(x_proj)\n",
    "    x_proj_recon = autoencoder.decode(x_proj_obs)\n",
    "\n",
    "    # Reconstruct final act\n",
    "    y_proj_obs = autoencoder.encode(y_proj)\n",
    "    y_proj_recon = autoencoder.decode(y_proj_obs)\n",
    "\n",
    "    pred_proj_obs = x_proj_obs @ torch.linalg.matrix_power(K_matrix, int(k_steps))\n",
    "    pred_proj = autoencoder.decode(pred_proj_obs)\n",
    "\n",
    "    if preprocess:\n",
    "        pred = undo_preprocessing_acts(pred_proj, preproc_dict, final_idx, device)\n",
    "    else:\n",
    "        pred = pred_proj\n",
    "\n",
    "    if preprocess:\n",
    "        y_recon = undo_preprocessing_acts(y_proj_recon, preproc_dict, final_idx, device)\n",
    "    else:\n",
    "        y_recon = y_proj_recon\n",
    "\n",
    "    if type(model) is ConvResNet:\n",
    "        pred = pred.reshape(-1, 512, 4, 4)\n",
    "        koopman_pred = torch.argmax(model.components[-2:](pred), dim=1)\n",
    "\n",
    "    else:\n",
    "        # Feed pred to classifier\n",
    "        koopman_pred = torch.argmax(model.components[-1:](pred), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetClass(config=dataset_config, root=data_root)\n",
    "model_metric = MulticlassAccuracy(num_classes=dataset.out_features)\n",
    "model_metric.update(model_pred, labels)\n",
    "\n",
    "koopman_metric = MulticlassAccuracy(num_classes=dataset.out_features)\n",
    "koopman_metric.update(koopman_pred[:batch_size], labels)\n",
    "\n",
    "print(\"Original accuracy: \", model_metric.compute())\n",
    "print(\"Koopman accuracy: \", koopman_metric.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(\n",
    "    original,\n",
    "    reconstructed,\n",
    "    reshape_dims=None,\n",
    "    height=400,\n",
    "    width=800,\n",
    "    titles=[\"Original\", \"Reconstructed\"],\n",
    "):\n",
    "    # Auto-calculate reshape dimensions if not provided\n",
    "    if reshape_dims is None:\n",
    "        total_elements = original.numel()\n",
    "        sqrt_elements = math.sqrt(total_elements)\n",
    "\n",
    "        if sqrt_elements.is_integer():\n",
    "            side = int(sqrt_elements)\n",
    "            reshape_dims = (side, side)\n",
    "        else:\n",
    "            side1 = int(math.sqrt(total_elements))\n",
    "            while total_elements % side1 != 0 and side1 > 1:\n",
    "                side1 -= 1\n",
    "\n",
    "            if side1 > 1:\n",
    "                side2 = total_elements // side1\n",
    "                reshape_dims = (side1, side2)\n",
    "            else:\n",
    "                reshape_dims = (1, total_elements)\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=titles)\n",
    "\n",
    "    for i, img in enumerate([original, reconstructed], 1):\n",
    "        fig.add_trace(px.imshow(img.reshape(reshape_dims)).data[0], row=1, col=i)\n",
    "\n",
    "    fig.update_layout(height=height, width=width, xaxis_scaleanchor=\"y\", xaxis2_scaleanchor=\"y2\")\n",
    "\n",
    "    error = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "    print(f\"Error: {error:.6f}\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = torch.randint(batch_size, (1,))[0].item()\n",
    "print(sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    x_proj[sample_idx].cpu(),\n",
    "    x_proj_recon[sample_idx].cpu(),\n",
    "    titles=[\"LoDim Input\", \"Recon. LoDim Input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    y_proj[sample_idx].cpu(),\n",
    "    y_proj_recon[sample_idx].cpu(),\n",
    "    titles=[\"LoDim Target\", \"Recon. LoDim Target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    y[sample_idx].flatten().cpu(), y_recon[sample_idx].cpu(), titles=[\"Original\", \"Recon\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    x_proj_obs[sample_idx].cpu(), y_proj_obs[sample_idx].cpu(), titles=[\"Obs Input\", \"Obs Target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    pred_proj_obs[sample_idx].cpu(),\n",
    "    y_proj_obs[sample_idx].cpu(),\n",
    "    titles=[\"Obs Predicted\", \"Obs Target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(\n",
    "    pred_proj[sample_idx].cpu(),\n",
    "    y_proj[sample_idx].cpu(),\n",
    "    titles=[\"Predicted LoDim\", \"LoDim Target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_images(pred[sample_idx].cpu(), y[sample_idx].cpu(), titles=[\"Predicted\", \"Target\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koopmann-hxjVWsls-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
